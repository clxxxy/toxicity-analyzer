{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97b7d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando o dataset...\n",
      "Dataset carregado com sucesso!\n",
      "   id                                         comentario  anotator1  \\\n",
      "0   1                                       Mais um lixo          1   \n",
      "1   2                    Essa nao tem vergonha na cara!!          1   \n",
      "2   3                     Essa mulher é doente.pilantra!          1   \n",
      "3   4                                Comunista safada...          1   \n",
      "4   5  Vagabunda. Comunista. Mentirosa. O povo chilen...          1   \n",
      "\n",
      "   anotator2  anotator3  label_final  \\\n",
      "0          1          1            1   \n",
      "1          1          1            1   \n",
      "2          1          1            1   \n",
      "3          1          1            1   \n",
      "4          1          1            1   \n",
      "\n",
      "                                 links_post    account_post  \n",
      "0  https://www.instagram.com/p/B2uThqdH9xI/  Carla Zambelli  \n",
      "1  https://www.instagram.com/p/B2uThqdH9xI/  Carla Zambelli  \n",
      "2  https://www.instagram.com/p/B2uThqdH9xI/  Carla Zambelli  \n",
      "3  https://www.instagram.com/p/B2uThqdH9xI/  Carla Zambelli  \n",
      "4  https://www.instagram.com/p/B2uThqdH9xI/  Carla Zambelli  \n",
      "\n",
      "Distribuição das classes:\n",
      "label_final\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# URL do dataset HateBR no GitHub\n",
    "DATASET_URL = 'https://raw.githubusercontent.com/franciellevargas/HateBR/main/dataset/HateBR.csv'\n",
    "\n",
    "# 1. Carregar o dataset\n",
    "print(\"Carregando o dataset...\")\n",
    "try:\n",
    "    df = pd.read_csv(DATASET_URL)\n",
    "    print(\"Dataset carregado com sucesso!\")\n",
    "    # Mostra as primeiras linhas e a distribuição das classes\n",
    "    print(df.head())\n",
    "    print(\"\\nDistribuição das classes:\")\n",
    "    print(df['label_final'].value_counts(normalize=True))\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o dataset: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e4e279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando pré-processamento...\n",
      "Dados divididos: 5600 para treino, 1400 para teste.\n",
      "Vetorizando o texto...\n",
      "Treinando o modelo de classificação...\n",
      "Modelo treinado com sucesso!\n",
      "\n",
      "Avaliando o modelo nos dados de teste...\n",
      "\n",
      "Acurácia: 0.8314285714285714\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Não Odioso       0.83      0.83      0.83       700\n",
      "      Odioso       0.83      0.83      0.83       700\n",
      "\n",
      "    accuracy                           0.83      1400\n",
      "   macro avg       0.83      0.83      0.83      1400\n",
      "weighted avg       0.83      0.83      0.83      1400\n",
      "\n",
      "\n",
      "--- MVP PRONTO PARA USO ---\n"
     ]
    }
   ],
   "source": [
    "# 2. Pré-processamento e Definição das variáveis\n",
    "print(\"\\nIniciando pré-processamento...\")\n",
    "# Para este MVP, a única limpeza será converter para minúsculas.\n",
    "# O TfidfVectorizer já lida com muita coisa.\n",
    "X = df['comentario'].str.lower()\n",
    "y = df['label_final']\n",
    "\n",
    "# 3. Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Dados divididos: {len(X_train)} para treino, {len(X_test)} para teste.\")\n",
    "\n",
    "# 4. Vetorização do texto usando TF-IDF\n",
    "print(\"Vetorizando o texto...\")\n",
    "vectorizer = TfidfVectorizer(max_features=5000) # Usamos as 5000 palavras mais relevantes\n",
    "\n",
    "# Aprende o vocabulário com os dados de treino e transforma os dados de treino\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Apenas transforma os dados de teste com o vocabulário já aprendido\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "# 5. Treinamento do modelo de Regressão Logística\n",
    "print(\"Treinando o modelo de classificação...\")\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_vect, y_train)\n",
    "print(\"Modelo treinado com sucesso!\")\n",
    "\n",
    "# 6. Avaliação do modelo\n",
    "print(\"\\nAvaliando o modelo nos dados de teste...\")\n",
    "y_pred = model.predict(X_test_vect)\n",
    "\n",
    "print(\"\\nAcurácia:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Não Odioso', 'Odioso']))\n",
    "\n",
    "# Agora que o modelo está treinado, podemos usá-lo.\n",
    "# Os objetos que precisamos salvar/usar para novas previsões são:\n",
    "# - `model` (o classificador)\n",
    "# - `vectorizer` (o vetorizador)\n",
    "\n",
    "print(\"\\n--- MVP PRONTO PARA USO ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c96fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_toxicidade(comentario: str, model, vectorizer) -> dict:\n",
    "    \"\"\"\n",
    "    Recebe um comentário e retorna a classificação de toxicidade\n",
    "    e a probabilidade de ser discurso de ódio.\n",
    "    \"\"\"\n",
    "    # 1. Aplicar o mesmo pré-processamento (minúsculas)\n",
    "    comentario_processado = comentario.lower()\n",
    "    \n",
    "    # 2. Vetorizar o comentário usando o vetorizador JÁ TREINADO\n",
    "    comentario_vect = vectorizer.transform([comentario_processado])\n",
    "    \n",
    "    # 3. Fazer a predição\n",
    "    predicao = model.predict(comentario_vect)\n",
    "    probabilidades = model.predict_proba(comentario_vect)\n",
    "    \n",
    "    # A probabilidade de ser discurso de ódio é a probabilidade da classe \"1\"\n",
    "    prob_odio = probabilidades[0][1]\n",
    "    \n",
    "    if predicao[0] == 1:\n",
    "        classificacao = \"Discurso de Ódio\"\n",
    "    else:\n",
    "        classificacao = \"Não é Discurso de Ódio\"\n",
    "        \n",
    "    return {\n",
    "        \"classificacao\": classificacao,\n",
    "        \"nivel_toxicidade\": f\"{prob_odio:.2%}\" # Formata como porcentagem\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e31c825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testando o MVP com novos comentários ---\n",
      "Comentário: 'Esses políticos são todos uns bandidos, tinham que sumir do mapa!'\n",
      "Resultado: {'classificacao': 'Discurso de Ódio', 'nivel_toxicidade': '88.90%'}\n",
      "\n",
      "Comentário: 'O jogo de futebol ontem foi muito emocionante, gostei bastante do resultado.'\n",
      "Resultado: {'classificacao': 'Não é Discurso de Ódio', 'nivel_toxicidade': '35.25%'}\n",
      "\n",
      "Digite um comentário para ser analisado (ou 'sair' para terminar):\n",
      "Resultado: {'classificacao': 'Não é Discurso de Ódio', 'nivel_toxicidade': '39.81%'}\n",
      "\n",
      "Resultado: {'classificacao': 'Não é Discurso de Ódio', 'nivel_toxicidade': '39.81%'}\n",
      "\n",
      "Resultado: {'classificacao': 'Não é Discurso de Ódio', 'nivel_toxicidade': '39.81%'}\n",
      "\n",
      "Resultado: {'classificacao': 'Não é Discurso de Ódio', 'nivel_toxicidade': '39.81%'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- EXEMPLOS DE USO ---\n",
    "print(\"\\n--- Testando o MVP com novos comentários ---\")\n",
    "\n",
    "# Exemplo 1: Comentário potencialmente tóxico\n",
    "comentario1 = \"Esses políticos são todos uns bandidos, tinham que sumir do mapa!\"\n",
    "resultado1 = avaliar_toxicidade(comentario1, model, vectorizer)\n",
    "print(f\"Comentário: '{comentario1}'\")\n",
    "print(f\"Resultado: {resultado1}\\n\")\n",
    "\n",
    "# Exemplo 2: Comentário neutro\n",
    "comentario2 = \"O jogo de futebol ontem foi muito emocionante, gostei bastante do resultado.\"\n",
    "resultado2 = avaliar_toxicidade(comentario2, model, vectorizer)\n",
    "print(f\"Comentário: '{comentario2}'\")\n",
    "print(f\"Resultado: {resultado2}\\n\")\n",
    "\n",
    "# Exemplo 3: Comentário inserido pelo usuário\n",
    "'''print(\"Digite um comentário para ser analisado (ou 'sair' para terminar):\")\n",
    "while True:\n",
    "    meu_comentario = input(\"> \")\n",
    "    if meu_comentario.lower() == 'sair':\n",
    "        break\n",
    "    resultado = avaliar_toxicidade(meu_comentario, model, vectorizer)\n",
    "    print(f\"Resultado: {resultado}\\n\")'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
