{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "97b7d796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregando o dataset...\n",
      "Dataset carregado com sucesso!\n",
      "   id                                         comentario  anotator1  \\\n",
      "0   1                                       Mais um lixo          1   \n",
      "1   2                    Essa nao tem vergonha na cara!!          1   \n",
      "2   3                     Essa mulher é doente.pilantra!          1   \n",
      "3   4                                Comunista safada...          1   \n",
      "4   5  Vagabunda. Comunista. Mentirosa. O povo chilen...          1   \n",
      "\n",
      "   anotator2  anotator3  label_final  \\\n",
      "0          1          1            1   \n",
      "1          1          1            1   \n",
      "2          1          1            1   \n",
      "3          1          1            1   \n",
      "4          1          1            1   \n",
      "\n",
      "                                 links_post    account_post  \n",
      "0  https://www.instagram.com/p/B2uThqdH9xI/  Carla Zambelli  \n",
      "1  https://www.instagram.com/p/B2uThqdH9xI/  Carla Zambelli  \n",
      "2  https://www.instagram.com/p/B2uThqdH9xI/  Carla Zambelli  \n",
      "3  https://www.instagram.com/p/B2uThqdH9xI/  Carla Zambelli  \n",
      "4  https://www.instagram.com/p/B2uThqdH9xI/  Carla Zambelli  \n",
      "\n",
      "Distribuição das classes:\n",
      "label_final\n",
      "1    0.5\n",
      "0    0.5\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Iniciando pré-processamento...\n",
      "Dados divididos: 5600 para treino, 1400 para teste.\n",
      "Vetorizando o texto...\n",
      "Treinando o modelo de classificação...\n",
      "Modelo treinado com sucesso!\n",
      "\n",
      "Avaliando o modelo nos dados de teste...\n",
      "\n",
      "Acurácia: 0.8314285714285714\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Não Odioso       0.83      0.83      0.83       700\n",
      "      Odioso       0.83      0.83      0.83       700\n",
      "\n",
      "    accuracy                           0.83      1400\n",
      "   macro avg       0.83      0.83      0.83      1400\n",
      "weighted avg       0.83      0.83      0.83      1400\n",
      "\n",
      "\n",
      "--- MVP PRONTO PARA USO ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# URL do dataset HateBR no GitHub\n",
    "DATASET_URL = 'https://raw.githubusercontent.com/franciellevargas/HateBR/main/dataset/HateBR.csv'\n",
    "\n",
    "# 1. Carregar o dataset\n",
    "print(\"Carregando o dataset...\")\n",
    "try:\n",
    "    df = pd.read_csv(DATASET_URL)\n",
    "    print(\"Dataset carregado com sucesso!\")\n",
    "    # Mostra as primeiras linhas e a distribuição das classes\n",
    "    print(df.head())\n",
    "    print(\"\\nDistribuição das classes:\")\n",
    "    print(df['label_final'].value_counts(normalize=True))\n",
    "except Exception as e:\n",
    "    print(f\"Erro ao carregar o dataset: {e}\")\n",
    "    exit()\n",
    "\n",
    "# 2. Pré-processamento e Definição das variáveis\n",
    "print(\"\\nIniciando pré-processamento...\")\n",
    "# Para este MVP, a única limpeza será converter para minúsculas.\n",
    "# O TfidfVectorizer já lida com muita coisa.\n",
    "X = df['comentario'].str.lower()\n",
    "y = df['label_final']\n",
    "\n",
    "# 3. Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "print(f\"Dados divididos: {len(X_train)} para treino, {len(X_test)} para teste.\")\n",
    "\n",
    "# 4. Vetorização do texto usando TF-IDF\n",
    "print(\"Vetorizando o texto...\")\n",
    "vectorizer = TfidfVectorizer(max_features=5000) # Usamos as 5000 palavras mais relevantes\n",
    "\n",
    "# Aprende o vocabulário com os dados de treino e transforma os dados de treino\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# Apenas transforma os dados de teste com o vocabulário já aprendido\n",
    "X_test_vect = vectorizer.transform(X_test)\n",
    "\n",
    "# 5. Treinamento do modelo de Regressão Logística\n",
    "print(\"Treinando o modelo de classificação...\")\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_vect, y_train)\n",
    "print(\"Modelo treinado com sucesso!\")\n",
    "\n",
    "# 6. Avaliação do modelo\n",
    "print(\"\\nAvaliando o modelo nos dados de teste...\")\n",
    "y_pred = model.predict(X_test_vect)\n",
    "\n",
    "print(\"\\nAcurácia:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Não Odioso', 'Odioso']))\n",
    "\n",
    "# Agora que o modelo está treinado, podemos usá-lo.\n",
    "# Os objetos que precisamos salvar/usar para novas previsões são:\n",
    "# - `model` (o classificador)\n",
    "# - `vectorizer` (o vetorizador)\n",
    "\n",
    "print(\"\\n--- MVP PRONTO PARA USO ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c96fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliar_toxicidade(comentario: str, model, vectorizer) -> dict:\n",
    "    \"\"\"\n",
    "    Recebe um comentário e retorna a classificação de toxicidade\n",
    "    e a probabilidade de ser discurso de ódio.\n",
    "    \"\"\"\n",
    "    # 1. Aplicar o mesmo pré-processamento (minúsculas)\n",
    "    comentario_processado = comentario.lower()\n",
    "    \n",
    "    # 2. Vetorizar o comentário usando o vetorizador JÁ TREINADO\n",
    "    comentario_vect = vectorizer.transform([comentario_processado])\n",
    "    \n",
    "    # 3. Fazer a predição\n",
    "    predicao = model.predict(comentario_vect)\n",
    "    probabilidades = model.predict_proba(comentario_vect)\n",
    "    \n",
    "    # A probabilidade de ser discurso de ódio é a probabilidade da classe \"1\"\n",
    "    prob_odio = probabilidades[0][1]\n",
    "    \n",
    "    if predicao[0] == 1:\n",
    "        classificacao = \"Discurso de Ódio\"\n",
    "    else:\n",
    "        classificacao = \"Não é Discurso de Ódio\"\n",
    "        \n",
    "    return {\n",
    "        \"classificacao\": classificacao,\n",
    "        \"nivel_toxicidade\": f\"{prob_odio:.2%}\" # Formata como porcentagem\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e31c825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testando o MVP com novos comentários ---\n",
      "Comentário: 'Esses políticos são todos uns bandidos, tinham que sumir do mapa!'\n",
      "Resultado: {'classificacao': 'Discurso de Ódio', 'nivel_toxicidade': '88.90%'}\n",
      "\n",
      "Comentário: 'O jogo de futebol ontem foi muito emocionante, gostei bastante do resultado.'\n",
      "Resultado: {'classificacao': 'Não é Discurso de Ódio', 'nivel_toxicidade': '35.25%'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- EXEMPLOS DE USO ---\n",
    "print(\"\\n--- Testando o MVP com novos comentários ---\")\n",
    "\n",
    "# Exemplo 1: Comentário potencialmente tóxico\n",
    "comentario1 = \"Esses políticos são todos uns bandidos, tinham que sumir do mapa!\"\n",
    "resultado1 = avaliar_toxicidade(comentario1, model, vectorizer)\n",
    "print(f\"Comentário: '{comentario1}'\")\n",
    "print(f\"Resultado: {resultado1}\\n\")\n",
    "\n",
    "# Exemplo 2: Comentário neutro\n",
    "comentario2 = \"O jogo de futebol ontem foi muito emocionante, gostei bastante do resultado.\"\n",
    "resultado2 = avaliar_toxicidade(comentario2, model, vectorizer)\n",
    "print(f\"Comentário: '{comentario2}'\")\n",
    "print(f\"Resultado: {resultado2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0b2b927a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Salvando o modelo e o vetorizador em arquivos...\n",
      "Modelo e vetorizador salvos com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# (Após o bloco de treinamento e avaliação do modelo...)\n",
    "import joblib\n",
    "\n",
    "print(\"\\nSalvando o modelo e o vetorizador em arquivos...\")\n",
    "\n",
    "# Salva o modelo treinado\n",
    "joblib.dump(model, 'modelo_odio.joblib')\n",
    "\n",
    "# Salva o vetorizador\n",
    "joblib.dump(vectorizer, 'vetorizador_odio.joblib')\n",
    "\n",
    "print(\"Modelo e vetorizador salvos com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78dff293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo e vetorizador carregados com sucesso.\n",
      "\n",
      "--- INICIANDO ANÁLISE DE TOXICIDADE NO REDDIT ---\n",
      "Conexão com a API do Reddit estabelecida. Modo Read-Only: True\n",
      "\n",
      "Buscando posts populares no r/brasil...\n",
      "Analisando comentários do post: 'Deputado Nikolas Ferreira recebendo o adesivo do DIVINO FUSCA CLUBE'\n",
      "\n",
      "--- Relatório Final da Análise ---\n",
      "  - Subreddit analisado: r/brasil\n",
      "  - Post: 'Deputado Nikolas Ferreira recebendo o adesivo do DIVINO FUSC...'\n",
      "  - Comentários analisados: 100\n",
      "  - Comentários classificados como discurso de ódio: 55\n",
      "  - Nível médio de toxicidade nos comentários: 53.29%\n",
      "\n",
      "==================================================\n",
      "\n",
      "--- Comentários Classificados como Discurso de Ódio ---\n",
      "1. MDS essa porra de águia. Kkkkkkkkk\n",
      "---\n",
      "2. Fomos de apito de cachorro pra corneta de cachorro muito rápido\n",
      "---\n",
      "3. Que lamentável ver o James Cameron ao lado desse verme, espero que Avatar 3 flope\n",
      "---\n",
      "4. Esse adesivo não vai ter como negar que é nazismo velado. Vão ter que usar a desculpa do “não sabia” ou “não vi”.\n",
      "---\n",
      "5. Clube de carro antigo e de motoqueiro é isso aí mesmo.\n",
      "---\n",
      "6. O Fusca dos caras:\n",
      "\n",
      "![gif](giphy|15L4GgZxncHh6)\n",
      "---\n",
      "7. porque os dois são os exemplos da raça ariana ne\n",
      "---\n",
      "8. Dá vontade de leitar esse cara, pqp\n",
      "---\n",
      "9. Só eu que tô vendo o neo-nazismo dessa bagaça ou mais alguém?\n",
      "---\n",
      "10. O nazipardo idoso da foto tá vivo? Pergunto porque não parece\n",
      "---\n",
      "11. isso já nem é mais apito de cachorro, é apito de godzilla\n",
      "---\n",
      "12. carai, latino nazista é a vaca a favor do açougueiro mesmo. Pior q se falar isso pra ele, vai pagar de ignorante, como sempre pq não precisa muita informação pra saber disso, e me dói dizer isso, mas ele é inteligente pra bater o olho e saber\n",
      "---\n",
      "13. Tem cheiro de bosta, cor de bosta e aspecto de bosta.\n",
      "\n",
      "Daí, sempre aparece um pra dizer: \"ah, nada a ver isso aí, você comeu, por acaso? Não?! Então não pode dizer que é uma bosta!\"\n",
      "---\n",
      "14. Dois pardos celebrando uma bizarrice, mundo louco bicho.\n",
      "---\n",
      "15. Divinopolitano aqui, não da pra esperar muito de uma cidade que 80% votou em Bolsonaro\n",
      "---\n",
      "16. Nem fodendo.\n",
      "---\n",
      "17. KKKKKKK Mno q porra É essa\n",
      "---\n",
      "18. Kkkkkkkkkkkkkkkkkkkkk a certeza de impunidade é tão grande que os cara nem escondem\n",
      "---\n",
      "19. Carai, além de tudo é Nazi... pqp.\n",
      "---\n",
      "20. Os cara perderam a vergonha msm eim pqp. Nem disfarçam mais.\n",
      "---\n",
      "21. Isso me lembra uma vez que foi num desses eventos de carro antigo, nunca vi tanto Fusca no lugar só, era um mais personalizado do que  o outro e de todas as cores possíveis. Foi muito legal.\n",
      "---\n",
      "22. Nem te conto de onde a Volks é kkkkk combinou com a águia\n",
      "---\n",
      "23. Só pode ser pegadinha não é possível\n",
      "---\n",
      "24. Obviamente o Nikolas é só um apaixonado por carros antigos, vocês que vêem problema em qualquer coisinha /s\n",
      "---\n",
      "25. Chupetinha de nazista,  Chupetinha do pcc...\n",
      "---\n",
      "26. Sei nem quantas camadas tem isso\n",
      "\n",
      "https://preview.redd.it/68dxje26q8mf1.jpeg?width=548&format=pjpg&auto=webp&s=0128423c415e7f195c3607a8305ee4c6a7ddf277\n",
      "---\n",
      "27. A origem da fabricante é nazi, o do carro fusca também. Se buscar no google por vintage wolkswagen logo. Vai achar esse símbolo ai.\n",
      "\n",
      "A única forma de apagar qualquer coisa nazi dessa marca é banindo ela de existir no brasil.\n",
      "---\n",
      "28. Que que a empresa do fusca tem a ver com nazismo?\n",
      "---\n",
      "29. Kkkkkkkkkkkkkkkkkkkkkk nem fodendo\n",
      "---\n",
      "30. PQP!!!!\n",
      "---\n",
      "31. Uma. Poxa nunca vou entender, um povo caboclo, mais mestiço impossível e acha que é branco o suficiente pra ser nazista. Enquanto na Alemanha, o povo com vergonha do passado e fazendo de tudo pra reparar e lembrar pra não recorrer no mesmo erro. Brasileiro assim além de burro é muito vira lata pra imitar até o que não presta.\n",
      "---\n",
      "32. Suástica é proibido? E o carro é uma porcaria obsoleta.\n",
      "---\n",
      "33. Clube de automóveis sempre foram abrigo de pau no cu. Hoje os clubes de tiro têm tomado seu lugar.\n",
      "---\n",
      "34. Chamou o bait. E o gado vem correndo mugir dar o que ele quer.\n",
      "---\n",
      "35. Tem que ser muito sensível pra se ofender com um adesivo desses.\n",
      "---\n",
      "36. Namoral vocês se esforçam muito pra odiar o cara\n",
      "---\n",
      "37. A águia, a empresa, o carro. É até cartunesco.\n",
      "---\n",
      "38. Porra, eu tava passando batido pqp\n",
      "---\n",
      "39. E não tem nem desculpa, dá pra criar um logo de clube de Fusca sem fazer nenhuma referência ao nazismo, tem milhares de exemplos.\n",
      "---\n",
      "40. Não é só a águia, fusca foi desenvolvido com envolvimento do hitler e woklsvagen apoiava o partido nazista.\n",
      "\n",
      "É um apito de cachorro, muito obviamente nazista.\n",
      "---\n",
      "41. Corneta de cadela\n",
      "---\n",
      "42. Porra kkkkkkkkkkkkkkkkkkkkkk\n",
      "---\n",
      "43. Dá para negar que é nazismo velado sim. \n",
      "\n",
      "\n",
      "Isso aí é nazismo escancarado\n",
      "---\n",
      "44. É lamentável, mas não vai dar em nada...até a Boy London (marca de roupas gringa) usa esse símbolo e ninguém fez nada até hoje.\n",
      "\n",
      "https://preview.redd.it/eb4nv72u18mf1.jpeg?width=416&format=pjpg&auto=webp&s=75e21c7b77b873397d0499f80a8416691990792d\n",
      "---\n",
      "45. Pq isso acontece? Qual a relação que faz com que a maioria desses clubes só tenham gente dessa laia?\n",
      "---\n",
      "46. Exatamente. Só tem pau no cu nesses clubes. Verdadeira escória da sociedade, que jamais teria posição de destaque em coisa alguma.\n",
      "---\n",
      "47. Olha eu entendi oq vc quis dizer e sem querer ser chato, mas usando meu hyperfoco em veículos militares pra levantar q esses tanks parecem ser soviéticos, quase ctz q é uma bt7.\n",
      "Infelizmente jogar warthunder fez isso comigo, virei buzologo de tank e avião kkkk\n",
      "---\n",
      "48. mas esse ai é um bt-5/7, tanque russo\n",
      "---\n",
      "49. Isso é um péssimo tanque, mas um otimo carro de corrida.\n",
      "---\n",
      "50. Esse aí impediu que  muitos Fritz não dirigissem fuscas  .\n",
      "---\n",
      "51. Tem nem como disfarçar\n",
      "---\n",
      "52. KKKKKKKKKK\n",
      "---\n",
      "53. Amigo, não se faz brincadeiras gostosas com esse tipo de gente não\n",
      "---\n",
      "54. Vontade de quê?\n",
      "---\n",
      "55. O da esquerda né?? Né????\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import joblib\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# --- CONFIGURAÇÃO ---\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\")\n",
    "\n",
    "# Carregar o modelo e o vetorizador salvos\n",
    "try:\n",
    "    model = joblib.load('modelo_odio.joblib')\n",
    "    vectorizer = joblib.load('vetorizador_odio.joblib')\n",
    "    print(\"Modelo e vetorizador carregados com sucesso.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"ERRO: Arquivos 'modelo_odio.joblib' ou 'vetorizador_odio.joblib' não encontrados.\")\n",
    "    print(\"Certifique-se de executar o script de treinamento ('analisador_odio.py') primeiro.\")\n",
    "    exit()\n",
    "\n",
    "# --- FUNÇÕES ---\n",
    "\n",
    "def avaliar_toxicidade(comentario: str) -> dict:\n",
    "    \"\"\"\n",
    "    Recebe um comentário e retorna a classificação de toxicidade\n",
    "    e a probabilidade de ser discurso de ódio.\n",
    "    \"\"\"\n",
    "    comentario_processado = comentario.lower()\n",
    "    comentario_vect = vectorizer.transform([comentario_processado])\n",
    "    predicao = model.predict(comentario_vect)\n",
    "    probabilidades = model.predict_proba(comentario_vect)\n",
    "    prob_odio = probabilidades[0][1]\n",
    "    \n",
    "    return {\n",
    "        \"eh_odio\": predicao[0] == 1,\n",
    "        \"nivel_toxicidade\": prob_odio\n",
    "    }\n",
    "\n",
    "# --- EXECUÇÃO PRINCIPAL ---\n",
    "\n",
    "def main():\n",
    "    print(\"\\n--- INICIANDO ANÁLISE DE TOXICIDADE NO REDDIT ---\")\n",
    "\n",
    "    # Conectar à API do Reddit\n",
    "    try:\n",
    "        reddit = praw.Reddit(\n",
    "            client_id=CLIENT_ID,\n",
    "            client_secret=CLIENT_SECRET,\n",
    "            user_agent=USER_AGENT,\n",
    "            check_for_async=False\n",
    "        )\n",
    "        print(f\"Conexão com a API do Reddit estabelecida. Modo Read-Only: {reddit.read_only}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao conectar com a API do Reddit: {e}\")\n",
    "        return\n",
    "\n",
    "    subreddit_alvo = \"brasil\"\n",
    "    limite_comentarios = 100\n",
    "\n",
    "    print(f\"\\nBuscando posts populares no r/{subreddit_alvo}...\")\n",
    "    \n",
    "    subreddit = reddit.subreddit(subreddit_alvo)\n",
    "    try:\n",
    "        post_popular = next(p for p in subreddit.hot(limit=5) if not p.stickied)\n",
    "        print(f\"Analisando comentários do post: '{post_popular.title}'\")\n",
    "    except StopIteration:\n",
    "        print(f\"Não foi possível encontrar um post válido no r/{subreddit_alvo}.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Ocorreu um erro ao buscar o post: {e}\")\n",
    "        return\n",
    "\n",
    "    total_toxicidade = 0\n",
    "    comentarios_odiosos = 0\n",
    "    comentarios_analisados = 0\n",
    "    \n",
    "    # NOVO: Lista para armazenar os comentários classificados como ódio\n",
    "    comentarios_odiosos_lista = []\n",
    "\n",
    "    post_popular.comments.replace_more(limit=0)\n",
    "\n",
    "    for comment in post_popular.comments.list():\n",
    "        if comentarios_analisados >= limite_comentarios:\n",
    "            break\n",
    "        \n",
    "        texto_comentario = comment.body\n",
    "        \n",
    "        if not texto_comentario or texto_comentario in ['[deleted]', '[removed]']:\n",
    "            continue\n",
    "\n",
    "        resultado = avaliar_toxicidade(texto_comentario)\n",
    "        total_toxicidade += resultado[\"nivel_toxicidade\"]\n",
    "        \n",
    "        if resultado[\"eh_odio\"]:\n",
    "            comentarios_odiosos += 1\n",
    "            # NOVO: Adiciona o texto do comentário à lista\n",
    "            comentarios_odiosos_lista.append(texto_comentario)\n",
    "        \n",
    "        comentarios_analisados += 1\n",
    "\n",
    "    media_toxicidade = (total_toxicidade / comentarios_analisados) if comentarios_analisados > 0 else 0\n",
    "    \n",
    "    print(\"\\n--- Relatório Final da Análise ---\")\n",
    "    print(f\"  - Subreddit analisado: r/{subreddit_alvo}\")\n",
    "    print(f\"  - Post: '{post_popular.title[:60]}...'\")\n",
    "    print(f\"  - Comentários analisados: {comentarios_analisados}\")\n",
    "    print(f\"  - Comentários classificados como discurso de ódio: {comentarios_odiosos}\")\n",
    "    print(f\"  - Nível médio de toxicidade nos comentários: {media_toxicidade:.2%}\")\n",
    "\n",
    "    # NOVO: Loop para imprimir os comentários ofensivos encontrados\n",
    "    print(\"\\n\" + \"=\"*50) # Adiciona uma linha separadora\n",
    "\n",
    "    if comentarios_odiosos_lista:\n",
    "        print(\"\\n--- Comentários Classificados como Discurso de Ódio ---\")\n",
    "        for i, comentario in enumerate(comentarios_odiosos_lista, 1):\n",
    "            print(f\"{i}. {comentario}\\n---\") # Adiciona um separador entre os comentários\n",
    "    else:\n",
    "        print(\"\\nNenhum comentário foi classificado como discurso de ódio nesta amostra.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
